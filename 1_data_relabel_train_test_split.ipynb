{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local path to github repository, change as you see fit\n",
    "WORKING_DIR = '/Users/ebyler/gh/dice_detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_list = glob.glob(WORKING_DIR+'/data/JPEGImages/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-label data from YOLO format\n",
    "I used labelImg (https://github.com/tzutalin/labelImg) to label the dice images; LabelImg can output annotations in either PascalVOC or YOLO format. I used the YOLO format, which generates boxes with (x, y , l, w) annotations, where x and y are the upper left corner of the box and l and w are the normalized box length and width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo_file(fname, dir_=WORKING_DIR+'/data/Annotations'):\n",
    "    '''\n",
    "    takes image filename and returns annotation filename\n",
    "    e.g. data/JPEGImages/IMG_XXX.jpg\n",
    "         data/Annotations/IMG_XXX.txt\n",
    "    '''\n",
    "    txt_file = fname.replace('.jpg', '.txt')\n",
    "    full_path = '/'.join([dir_, txt_file])\n",
    "    if os.path.isfile(full_path):\n",
    "        return full_path\n",
    "    else:\n",
    "        print('ERROR')\n",
    "        return None\n",
    "\n",
    "def return_yolo_data(jpg_fname, ann_dir=WORKING_DIR+'/data/Annotations'):\n",
    "    '''\n",
    "    Reads annotation file, returns name and data\n",
    "    '''\n",
    "    base_fname = jpg_fname.split('/')[-1] # no path\n",
    "    txt_fname = get_yolo_file(base_fname, dir_=ann_dir)\n",
    "    data = np.genfromtxt(txt_fname, names='class,x,y,w,h')\n",
    "    # don't fail if there is only one labeled object in an image\n",
    "    data = np.atleast_1d(data)\n",
    "    # keep file name associated with each label\n",
    "    fname_arr = [base_fname]*data.shape[0]\n",
    "    return fname_arr, data\n",
    "\n",
    "def str_class(val):\n",
    "    '''\n",
    "    Return string label for index. \n",
    "    YOLO is zero indexed.\n",
    "    '''\n",
    "    if val == 0:\n",
    "        return 'one'\n",
    "    elif val == 1:\n",
    "        return 'two'\n",
    "    elif val == 2:\n",
    "        return 'three'\n",
    "    elif val == 3:\n",
    "        return 'four'\n",
    "    elif val == 4:\n",
    "        return 'five'\n",
    "    elif val == 5:\n",
    "        return 'six'\n",
    "\n",
    "def create_image_dict(image_list, ann_dir=WORKING_DIR+'/data/Annotations'):\n",
    "    '''\n",
    "    For input list of images, return a dictionary with label information.\n",
    "    dictionary keys: image file, class (string), x, y, h, w \n",
    "    '''\n",
    "    # empty lists for dictionary\n",
    "    fnames, classes = [], []\n",
    "    x,y,h,w = [],[],[],[]\n",
    "    for jpg_fname in image_list:\n",
    "        fs, data = return_yolo_data(jpg_fname, ann_dir=ann_dir)\n",
    "        for f, d in zip(fs, data):\n",
    "            fnames.append(f)\n",
    "            classes.append(str_class(d['class']))\n",
    "            x.append(d['x'])\n",
    "            y.append(d['y'])\n",
    "            h.append(d['h'])\n",
    "            w.append(d['w'])\n",
    "    out_dict = {'filename':fnames, 'class':classes, 'x':x, 'y':y, 'h':h, 'w':w}\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually create the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = create_image_dict(full_image_list)\n",
    "\n",
    "# create pandas dataFrame for easier manipulation\n",
    "full_labels = pd.DataFrame.from_dict(label_dict)\n",
    "\n",
    "# all images have been resized to 640x640 pixels\n",
    "# so normalized height and width are relative to this\n",
    "# (surprisingly easy task in apple's \"preview\", but could be done with PIL)\n",
    "full_labels['width'] = 640.\n",
    "full_labels['height'] = 640."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO format > TFRecord-like values\n",
    "YOLO format: (x,y) is the normalized coordinate of the upper left box corner.\n",
    "TFRecords wants the un-normalized values of the xmin, xmax, ymin, ymax for each box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      filename class         x         y         h         w  \\\n",
       "0     IMG_20191209_100348.jpg   one  0.558480  0.641447  0.066520  0.069444   \n",
       "1     IMG_20191209_100348.jpg   one  0.469846  0.177266  0.062135  0.063231   \n",
       "2     IMG_20191209_100348.jpg   two  0.587171  0.387244  0.067617  0.069810   \n",
       "3     IMG_20191209_100348.jpg  four  0.625548  0.468567  0.053363  0.055190   \n",
       "4     IMG_20191209_100348.jpg  four  0.428363  0.547332  0.059576  0.059942   \n",
       "...                       ...   ...       ...       ...       ...       ...   \n",
       "1790  IMG_20191209_095809.jpg   two  0.653692  0.365680  0.059576  0.065424   \n",
       "1791  IMG_20191209_095809.jpg  four  0.190789  0.387792  0.055556  0.065058   \n",
       "1792  IMG_20191209_095809.jpg  five  0.341923  0.430373  0.061038  0.057383   \n",
       "1793  IMG_20191209_095809.jpg  five  0.625548  0.245980  0.062135  0.059576   \n",
       "1794  IMG_20191209_095809.jpg   six  0.378655  0.215461  0.063231  0.066520   \n",
       "\n",
       "      width  height    xmin    xmax    ymin    ymax  \n",
       "0     640.0   640.0  335.21  379.65  388.30  432.75  \n",
       "1     640.0   640.0  280.47  320.94   93.22  133.68  \n",
       "2     640.0   640.0  353.45  398.13  225.50  270.18  \n",
       "3     640.0   640.0  382.69  418.01  282.22  317.54  \n",
       "4     640.0   640.0  254.97  293.33  331.11  369.47  \n",
       "...     ...     ...     ...     ...     ...     ...  \n",
       "1790  640.0   640.0  397.43  439.30  213.10  254.97  \n",
       "1791  640.0   640.0  101.29  142.92  227.37  269.01  \n",
       "1792  640.0   640.0  200.47  237.19  257.08  293.80  \n",
       "1793  640.0   640.0  381.29  419.42  138.36  176.49  \n",
       "1794  640.0   640.0  221.05  263.63  116.61  159.18  \n",
       "\n",
       "[1795 rows x 12 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate xmin, xmax, multiply by box width (640)\n",
    "full_labels['xmin'] = full_labels['width']*(full_labels['x'] - (full_labels['w']/2.0))\n",
    "full_labels['xmax'] = full_labels['width']*(full_labels['x'] + (full_labels['w']/2.0))\n",
    "# calculate ymin, ymax, multiply by box height (640)\n",
    "full_labels['ymin'] = full_labels['height']*(full_labels['y'] - (full_labels['w']/2.0))\n",
    "full_labels['ymax'] = full_labels['height']*(full_labels['y'] + (full_labels['w']/2.0))\n",
    "\n",
    "# I like it when these things print with a reasonable number of significant figures\n",
    "full_labels['ymin'] = full_labels['ymin'].map(lambda x: '%3.2f' % x)\n",
    "full_labels['ymax'] = full_labels['ymax'].map(lambda x: '%3.2f' % x)\n",
    "full_labels['xmin'] = full_labels['xmin'].map(lambda x: '%3.2f' % x)\n",
    "full_labels['xmax'] = full_labels['xmax'].map(lambda x: '%3.2f' % x)\n",
    "\n",
    "# take a peak at what the dataFrame looks like\n",
    "full_labels.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training and a test (val) set\n",
    "Each dice image has between 1 and 25 dice. We don't actually want to do a raw separation of our data based on the list of labeled objects - we want to group them by the filename.\n",
    "Thankfully, pandas makes this quite easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     52\n",
       "5     39\n",
       "7     29\n",
       "4     23\n",
       "3     14\n",
       "1     13\n",
       "10    12\n",
       "8     11\n",
       "2     10\n",
       "9      8\n",
       "11     8\n",
       "12     7\n",
       "22     7\n",
       "14     3\n",
       "24     2\n",
       "15     2\n",
       "16     2\n",
       "18     2\n",
       "21     2\n",
       "25     2\n",
       "13     1\n",
       "19     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group dataframe by filename\n",
    "grouped = full_labels.groupby('filename')\n",
    "\n",
    "# see the relative frequency of dice counts per image\n",
    "# Most of my images (52) had 6 dice.\n",
    "# 2 images had 25 dice in them (!)\n",
    "grouped.apply(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = full_labels.groupby('filename')\n",
    "grouped_list = [gb.get_group(x) for x in gb.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an 80/20 split for the training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An 0.8/0.2 split on 250 images: \n",
      "Train: 200\n",
      "Test: 50\n"
     ]
    }
   ],
   "source": [
    "# total number of images\n",
    "gl_len = len(grouped_list)\n",
    "\n",
    "# I want 80% of the images in the training set\n",
    "desired_split = 0.8\n",
    "\n",
    "train_len = int(gl_len*desired_split)\n",
    "\n",
    "# the test length is then 1-0.8=0.2\n",
    "# this way is just easier to ignore fractional images\n",
    "test_len = gl_len - train_len\n",
    "\n",
    "print('An {0:.1f}/{1:.1f} split on {2} images: '.format(desired_split, 1.-desired_split, gl_len))\n",
    "print('Train: {}'.format(train_len))\n",
    "print('Test: {}'.format(test_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly choose train_len images from the original image list. The test indices will then be the remaining images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double-check that size of train/test inds are as expected:\n",
      "200 50\n"
     ]
    }
   ],
   "source": [
    "train_index = np.random.choice(len(grouped_list), size=train_len, replace=False)\n",
    "test_index = np.setdiff1d(list(range(gl_len)), train_index)\n",
    "\n",
    "print('Double-check that size of train/test inds are as expected:')\n",
    "print(len(train_index), len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects in the train/test sets:\n",
      "1487 308\n"
     ]
    }
   ],
   "source": [
    "# create individual dataFrames for the test/train sets:\n",
    "# take first N files in index lists\n",
    "train = pd.concat([grouped_list[i] for i in train_index])\n",
    "test = pd.concat([grouped_list[i] for i in test_index])\n",
    "\n",
    "# Print the number of objects in each set.\n",
    "# This will be different than the total number of images\n",
    "# in each set, since images have more than one object.\n",
    "print('Total number of objects in the train/test sets:')\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print train/test sets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = WORKING_DIR+'/data/ImageSets/Main'\n",
    "cols = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "train.to_csv(SAVE_DIR+'/train_labels.csv', index=None, columns=cols)\n",
    "test.to_csv(SAVE_DIR+'/test_labels.csv', index=None, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
